{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c1f268a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johan\\AppData\\Local\\Temp\\ipykernel_24208\\636405232.py:6: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df1[\"label\"].replace(\"Mixed\", \"NO_STATEMENT\", inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NO_STATEMENT', 'FALSE', 'TRUE', 'MIXED']\n",
      "       id         label              label_vec\n",
      "0  703265  NO_STATEMENT  [0.98, 0.0, 0.0, 0.0]\n",
      "1  807621  NO_STATEMENT  [0.93, 0.0, 0.0, 0.0]\n",
      "2  703273  NO_STATEMENT  [0.98, 0.0, 0.0, 0.0]\n",
      "3  703274  NO_STATEMENT  [0.98, 0.0, 0.0, 0.0]\n",
      "4  703305  NO_STATEMENT  [0.98, 0.0, 0.0, 0.0]\n",
      "       id         label          label_vec\n",
      "0  807621  NO_STATEMENT  [0.02, 0.08, 0.9]\n",
      "1  703306  NO_STATEMENT  [0.05, 0.05, 0.9]\n",
      "2  703308         FALSE  [0.05, 0.6, 0.35]\n",
      "3  703394  NO_STATEMENT  [0.15, 0.15, 0.7]\n",
      "4  703282  NO_STATEMENT  [0.02, 0.08, 0.9]\n",
      "Index(['id', 'label', 'label_vec', 'gpt_label', 'gpt_label_vec'], dtype='object')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     41\u001b[39m df4 = pd.read_csv(\u001b[33m'\u001b[39m\u001b[33m../Data/OriginalData/truths.tsv\u001b[39m\u001b[33m'\u001b[39m, sep=\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     42\u001b[39m df4 = df4[[\u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtimestamp\u001b[39m\u001b[33m\"\u001b[39m]]\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m df5 = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m../Data/OriginalData/truthsocial_user_features.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\johan\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\johan\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\johan\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[39m, in \u001b[36mTextFileReader.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m   1916\u001b[39m nrows = validate_integer(\u001b[33m\"\u001b[39m\u001b[33mnrows\u001b[39m\u001b[33m\"\u001b[39m, nrows)\n\u001b[32m   1917\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1918\u001b[39m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[32m   1919\u001b[39m     (\n\u001b[32m   1920\u001b[39m         index,\n\u001b[32m   1921\u001b[39m         columns,\n\u001b[32m   1922\u001b[39m         col_dict,\n\u001b[32m-> \u001b[39m\u001b[32m1923\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[32m   1924\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[32m   1925\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1926\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1927\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\johan\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[39m, in \u001b[36mCParserWrapper.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    233\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.low_memory:\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m         chunks = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    235\u001b[39m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[32m    236\u001b[39m         data = _concatenate_chunks(chunks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:838\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader.read_low_memory\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:905\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._read_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:874\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:891\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:2053\u001b[39m, in \u001b[36mpandas._libs.parsers.raise_parser_error\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen codecs>:334\u001b[39m, in \u001b[36mgetstate\u001b[39m\u001b[34m(self)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from httpx import head\n",
    "import pandas as pd\n",
    "from regex import D\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "df1 = pd.read_csv('../Data/OriginalData/silver_labels_Timestamp5_7.csv')#chat gpt labels old\n",
    "df1[\"label\"].replace(\"Mixed\", \"NO_STATEMENT\", inplace=True)\n",
    "statement_cats = (df1[\"label\"].dropna().astype(str).unique().tolist())\n",
    "print(statement_cats)#what does mixed stand for?\n",
    "statement_enc = OneHotEncoder(categories=[statement_cats], sparse_output=False, handle_unknown=\"ignore\")\n",
    "statement_enc.fit(df1.loc[df1[\"label\"].notna(), [\"label\"]])\n",
    "onehot = statement_enc.transform(df1[[\"label\"]])\n",
    "conf = df1[\"confidence\"].fillna(0).to_numpy().reshape(-1, 1)\n",
    "df1[\"label_vec\"] = (onehot * conf).tolist()\n",
    "df1.drop(columns=[\"confidence\"], inplace=True)\n",
    "print(df1.head())\n",
    "df2 = pd.read_csv('../Data/OriginalData/truth_labels_prefilterd_gpt5.csv')#chat gpt labels after sentiment classification \n",
    "df2[\"label_vec\"] = df2[[\"TRUE\", \"FALSE\", \"NO_STATEMENT\"]].values.tolist()\n",
    "df2.rename(columns={\"truth_label\": \"label\"}, inplace=True)\n",
    "df2.drop(columns=[\"Unnamed: 0\", \"TRUE\", \"FALSE\", \"NO_STATEMENT\"], inplace=True)\n",
    "print(df2.head())\n",
    "\n",
    "df3 = pd.read_csv('../Data/OriginalData/truth_labels_fnn_gpt5.csv')\n",
    "df3.rename(columns={\"0\": \"id\",\"1\":\"label\"}, inplace=True)\n",
    "style_cluster_cats = (df3[\"label\"].dropna().astype(str).unique().tolist())\n",
    "style_enc = OneHotEncoder(categories=[style_cluster_cats], sparse_output=False, handle_unknown=\"ignore\")\n",
    "style_enc.fit(df3.loc[df3[\"label\"].notna(), [\"label\"]])\n",
    "onehot = style_enc.transform(df3[[\"label\"]])\n",
    "df3[\"label_vec\"] = onehot.tolist()\n",
    "\n",
    "\n",
    "df6 = df2.merge(df1, on=\"id\", how=\"outer\",)\n",
    "df6 = df6.merge(df3, on=\"id\", how=\"outer\")\n",
    "df6[\"gpt_label\"] = df6[\"label_x\"].combine_first(df6[\"label_y\"]).combine_first(df6[\"label\"]) \n",
    "df6[\"gpt_label_vec\"] = df6[\"label_vec_x\"].combine_first(df6[\"label_vec_y\"]).combine_first(df6[\"label_vec\"])\n",
    "df6 = df6.drop(columns=[\"label_x\", \"label_y\", \"label_vec_x\", \"label_vec_y\"])\n",
    "\n",
    "print(df6.columns)\n",
    "\n",
    "\n",
    "\n",
    "df4 = pd.read_csv('../Data/OriginalData/truths.tsv', sep=\"\\t\")\n",
    "df4 = df4[[\"id\", \"timestamp\"]]\n",
    "df5 = pd.read_csv('../Data/OriginalData/truthsocial_user_features.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0feb34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75131\n",
      "25020\n",
      "85300\n",
      "164733\n"
     ]
    }
   ],
   "source": [
    "print(len(df1))\n",
    "print(len(df2))\n",
    "print(len(df3))\n",
    "print(len(df6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439ccdae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80723\n",
      "80727\n",
      "Index(['id', 'timestamp_id', 'author', 'like_count', 'retruth_count',\n",
      "       'reply_count', 'text', 'hate_pred', 'hate_prob', 'sentiment_id',\n",
      "       'sentiment_conf', 'sentiment', 'statement_flag',\n",
      "       'statement_probability', 'clusters', 'style_cluster',\n",
      "       'activity_cluster', 'temporal_rhythm_cluster', 'topic_label',\n",
      "       'bert_label_vec', 'gpt_label', 'gpt_label_vec', 'timestamp'],\n",
      "      dtype='object')\n",
      "Index(['id', 'gpt_label', 'gpt_label_vec'], dtype='object')\n",
      "(1252,)\n"
     ]
    }
   ],
   "source": [
    "from httpx import head\n",
    "from polars import col\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "df5 = df5.rename(columns={\"timestamp\": \"timestamp_id\"})\n",
    "try:\n",
    "    df5[\"bert_label_vec\"] = df5[[\"TRUE\", \"FALSE\", \"NO_STMT\"]].values.tolist()\n",
    "    df5 = df5.drop(columns=[\"TRUTH_CLASS_x\",\"NO_STMT\",\"TRUE\" ,\"FALSE\"])#bert classifier output  \n",
    "except:\n",
    "    pass\n",
    "#try:\n",
    "    #df2 = df2.drop(columns=[\"Unnamed: 0\"])\n",
    "#except:\n",
    "#    pass\n",
    "df3 = df3.rename(columns={\"0\": \"id\",\"1\": \"truth_label_fnn\"})\n",
    "#df1 = df1.rename(columns=lambda c: c if c == \"id\" or c.startswith(\"silver_label_\") else f\"silver_label_{c}\")#these are chatgpt labels \n",
    "#try:\n",
    "#    df2[\"bert_prediction\"] = df2[[\"TRUE\", \"FALSE\", \"NO_STATEMENT\"]].values.tolist()#these are also chat gpt labels \n",
    "#    df2.drop(columns=[\"TRUE\", \"FALSE\", \"NO_STATEMENT\",\"truth_label\"], inplace=True)\n",
    "#except:\n",
    "#    pass\n",
    "print(len(df6))\n",
    "df = df5.merge(df6, on=\"id\", how=\"inner\")\n",
    "print(len(df))\n",
    "df[\"bert_label_vec\"] = df[\"bert_label_vec\"].apply(\n",
    "    lambda x: [0 if pd.isna(v) else v for v in x] if isinstance(x, list) else x\n",
    ")\n",
    "df=df.merge(df4, on=\"id\", how=\"inner\")\n",
    "#df = df.merge(df2, on=\"id\", how=\"inner\")\n",
    "#df = df.merge(df3, on=\"id\", how=\"inner\")\n",
    "print(df.columns)\n",
    "print(df6.columns)\n",
    "\n",
    "\n",
    "print(df[\"author\"].unique().shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2d615a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    5.0\n",
      "1    2.0\n",
      "2    3.0\n",
      "3    3.0\n",
      "4    3.0\n",
      "5    3.0\n",
      "6    3.0\n",
      "7    3.0\n",
      "8    3.0\n",
      "9    3.0\n",
      "Name: activity_cluster, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df[\"activity_cluster\"][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5a2613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Imports\n",
    "# =========================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "TEXT_DIM = 64          # node feature size for GNN\n",
    "MAX_LEN =2048\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# =========================\n",
    "# BERT + FFN Encoder\n",
    "# =========================\n",
    "class BertFFNEncoder(nn.Module):\n",
    "    def __init__(self, model_name, out_dim):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        \n",
    "        # freeze BERT\n",
    "        for p in self.bert.parameters():\n",
    "            p.requires_grad = False\n",
    "        \n",
    "        hidden_dim = self.bert.config.hidden_size\n",
    "        \n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        with torch.no_grad():\n",
    "            out = self.bert(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            cls_embedding = out.last_hidden_state[:, 0]  # [CLS]\n",
    "        \n",
    "        return self.ffn(cls_embedding)\n",
    "\n",
    "# =========================\n",
    "# Tokenizer & Model\n",
    "# =========================\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "encoder = BertFFNEncoder(MODEL_NAME, TEXT_DIM).to(DEVICE)\n",
    "encoder.eval()\n",
    "\n",
    "# =========================\n",
    "# Encode df[\"text\"]\n",
    "# =========================\n",
    "def encode_texts(texts, batch_size=32):\n",
    "    features = []\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "\n",
    "        enc = tokenizer(\n",
    "            batch,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=MAX_LEN,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        input_ids = enc[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = enc[\"attention_mask\"].to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            vecs = encoder(input_ids, attention_mask)\n",
    "\n",
    "        features.append(vecs.cpu().numpy())\n",
    "\n",
    "    return np.vstack(features)\n",
    "\n",
    "# =========================\n",
    "# RUN ENCODING\n",
    "# =========================\n",
    "text_features = encode_texts(df[\"text\"].tolist())\n",
    "\n",
    "# =========================\n",
    "# STORE AS NODE FEATURES\n",
    "# =========================\n",
    "#feature_df[\"text_feat\"] = text_features.tolist()\n",
    "\n",
    "# =========================\n",
    "# SANITY CHECKS\n",
    "# =========================\n",
    "#print(\"Node feature dim:\", len(feature_df[\"text_feat\"].iloc[0]))\n",
    "#print(\"Mean:\", np.mean(feature_df[\"text_feat\"].iloc[0]))\n",
    "#print(\"Std:\", np.std(feature_df[\"text_feat\"].iloc[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37c2ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#print(feature_df[\"text_feat\"][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3cd6a6",
   "metadata": {},
   "source": [
    "known Columns: [id, timestamp, author, like_count, retruth_count, reply_count, text, hate_pred, hate_prob, sentiment_id, sentiment_conf, sentiment, style_cluster, activity_cluster, temporal_rhythm_cluster, topic_label, silver_label_label, silver_label_confidence, bert_prediction, truth_label_fnn]\n",
    "\n",
    "Unknown Columns: [ statement_flag, statement_probability, TRUTH_CLASS_x, NO_STMT, TRUE, FALSE, clusters,]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853d0d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'timestamp_id', 'author', 'like_count', 'retruth_count',\n",
      "       'reply_count', 'text', 'hate_pred', 'hate_prob', 'sentiment_id',\n",
      "       'sentiment_conf', 'sentiment', 'statement_flag',\n",
      "       'statement_probability', 'clusters', 'style_cluster',\n",
      "       'activity_cluster', 'temporal_rhythm_cluster', 'topic_label',\n",
      "       'bert_label_vec', 'gpt_label', 'gpt_label_vec', 'timestamp'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3816e644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0.0', '1.0', '2.0', '3.0', '4.0', '5.0', '6.0', '7.0']\n"
     ]
    }
   ],
   "source": [
    "from cv2 import norm\n",
    "import numpy as np\n",
    "from transformers import  AutoTokenizer\n",
    "from yaml import Token\n",
    "\n",
    "feature_df= df[[\"id\",\"timestamp_id\",\"timestamp\",\"author\", 'bert_label_vec',\"gpt_label\",\"gpt_label_vec\"]].copy()\n",
    "feature_df[\"text_feat\"] = text_features.tolist()\n",
    "\n",
    "temporal_rhythm_cluster_cats = (df[\"temporal_rhythm_cluster\"].dropna().astype(str).unique().tolist())\n",
    "style_cluster_cats = (df[\"style_cluster\"].dropna().astype(str).unique().tolist())\n",
    "activity_cluster_cats = (df[\"activity_cluster\"].dropna().astype(str).unique().tolist())\n",
    "topic_label_cats = (df[\"topic_label\"].dropna().astype(str).unique().tolist())\n",
    "sentiment_cats = (df[\"sentiment_id\"].dropna().astype(str).unique().tolist())\n",
    "sortcats = sorted(style_cluster_cats)\n",
    "print(sortcats)\n",
    "\n",
    "\n",
    "style_enc = OneHotEncoder(categories=[sortcats], sparse_output=False, handle_unknown=\"ignore\")\n",
    "style_enc.fit(df.loc[df[\"style_cluster\"].notna(), [\"style_cluster\"]])\n",
    "onehot = style_enc.transform(df[[\"style_cluster\"]])\n",
    "feature_df[\"style_cluster_cats\"] = onehot.tolist()\n",
    "\n",
    "topic_label_enc = OneHotEncoder(categories=[topic_label_cats], sparse_output=False, handle_unknown=\"ignore\")\n",
    "topic_label_enc.fit(df.loc[df[\"topic_label\"].notna(), [\"topic_label\"]])\n",
    "onehot = topic_label_enc.transform(df[[\"topic_label\"]])\n",
    "feature_df[\"topic_label_cats\"] = onehot.tolist()\n",
    "\n",
    "topic_label_enc = OneHotEncoder(categories=[topic_label_cats], sparse_output=False, handle_unknown=\"ignore\")\n",
    "topic_label_enc.fit(df.loc[df[\"topic_label\"].notna(), [\"topic_label\"]])\n",
    "onehot = topic_label_enc.transform(df[[\"topic_label\"]])\n",
    "feature_df[\"topic_label_cats\"] = onehot.tolist()\n",
    "\n",
    "\n",
    "sentiment_cats = sorted(sentiment_cats)\n",
    "setiment_enc = OneHotEncoder(categories=[sentiment_cats], sparse_output=False, handle_unknown=\"ignore\")\n",
    "setiment_enc.fit(df.loc[df[\"sentiment_id\"].notna(), [\"sentiment_id\"]])\n",
    "onehot = setiment_enc.transform(df[[\"sentiment_id\"]])\n",
    "conf = df[\"sentiment_conf\"].fillna(0).to_numpy().reshape(-1, 1)\n",
    "feature_df[\"sentiment_vec\"] = (onehot * conf).tolist()\n",
    "\n",
    "activity_cluster_cats = sorted(activity_cluster_cats)\n",
    "activity_cluster_enc = OneHotEncoder(categories=[activity_cluster_cats], sparse_output=False, handle_unknown=\"ignore\")\n",
    "activity_cluster_enc.fit(df.loc[df[\"activity_cluster\"].notna(), [\"activity_cluster\"]])         \n",
    "onehot = activity_cluster_enc.transform(df[[\"activity_cluster\"]])\n",
    "feature_df[\"activity_cluster_cats\"] = onehot.tolist()\n",
    "\n",
    "temporal_rhythm_cluster_cats = sorted(df[\"temporal_rhythm_cluster\"].dropna().astype(str).unique().tolist())\n",
    "temporal_rhythm_cluster_enc = OneHotEncoder(categories=[temporal_rhythm_cluster_cats], sparse_output=False, handle_unknown=\"ignore\")\n",
    "temporal_rhythm_cluster_enc.fit(df.loc[df[\"temporal_rhythm_cluster\"].notna(), [\"temporal_rhythm_cluster\"]])\n",
    "onehot = temporal_rhythm_cluster_enc.transform(df[[\"temporal_rhythm_cluster\"]])\n",
    "feature_df[\"temporal_rhythm_cluster_cats\"] = onehot.tolist()\n",
    "\n",
    "\n",
    "feature_df[\"hate_vec\"] = df[\"hate_prob\"]\n",
    "\n",
    "statement_cats = (df[\"statement_flag\"].dropna().astype(str).unique().tolist())\n",
    "statement_enc = OneHotEncoder(categories=[statement_cats], sparse_output=False, handle_unknown=\"ignore\")\n",
    "statement_enc.fit(df.loc[df[\"statement_flag\"].notna(), [\"statement_flag\"]])\n",
    "onehot = statement_enc.transform(df[[\"statement_flag\"]])\n",
    "conf = df[\"statement_probability\"].fillna(0).to_numpy().reshape(-1, 1)\n",
    "feature_df[\"statement_vec\"] = (onehot * conf).tolist()\n",
    "\n",
    "\n",
    "x = np.log1p(df[\"like_count\"])\n",
    "\n",
    "mean = x.mean()\n",
    "std = x.std()\n",
    "\n",
    "feature_df[\"like_count_scaled\"] = (x - mean) / std\n",
    "\n",
    "x = np.log1p(df[\"retruth_count\"])\n",
    "mean = x.mean()\n",
    "std = x.std()\n",
    "feature_df[\"retruth_count_scaled\"] = (x - mean) / std\n",
    "\n",
    "x = np.log1p(df[\"reply_count\"]) \n",
    "mean = x.mean()\n",
    "std = x.std()\n",
    "feature_df[\"reply_count_scaled\"] = (x - mean) / std\n",
    "\n",
    "#TODO: i need to deifine a dictionary \n",
    "#Tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "#feature_df[\"text_tokenized\"] = feature_df[\"id\"].apply(lambda x: Tokenizer.encode(df.loc[df[\"id\"] == x, \"text\"].values[0],))\n",
    "\n",
    "#feature_df[\"hate_prob\"] = df[\"hate_prob\"]\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# parse clusters\n",
    "parse = lambda x: [] if x is None or x != x else (\n",
    "    x if isinstance(x, list)\n",
    "    else [int(c) for c in str(x).strip(\"[]\").split(\",\") if c.strip()]\n",
    ")\n",
    "\n",
    "clusters = df[\"clusters\"].apply(parse)\n",
    "\n",
    "# vocab\n",
    "cats = sorted({c for row in clusters for c in row})\n",
    "idx = {c: i for i, c in enumerate(cats)}\n",
    "\n",
    "# encode + normalize\n",
    "feature_df[\"cluster_vec\"] = clusters.apply(\n",
    "    lambda row: (\n",
    "        lambda v: v / v.sum() if v.sum() > 0 else v\n",
    "    )(np.bincount([idx[c] for c in row], minlength=len(cats)).astype(float))\n",
    ").tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b304f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df.to_csv(\"../Data/ProcessedData/network_modeling_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae3abd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'timestamp_id', 'timestamp', 'author', 'bert_label_vec',\n",
      "       'gpt_label', 'gpt_label_vec', 'text_feat', 'style_cluster_cats',\n",
      "       'topic_label_cats', 'sentiment_vec', 'activity_cluster_cats',\n",
      "       'temporal_rhythm_cluster_cats', 'hate_vec', 'statement_vec',\n",
      "       'like_count_scaled', 'retruth_count_scaled', 'reply_count_scaled',\n",
      "       'cluster_vec'],\n",
      "      dtype='object')\n",
      "       id  timestamp_id            timestamp  author  \\\n",
      "0  703265            13  2022-05-14 00:00:00    2247   \n",
      "1  807621             6  2022-03-26 00:00:00   20054   \n",
      "2  703273            25  2022-08-01 00:00:00   11225   \n",
      "3  703274            25  2022-08-01 00:00:00   11225   \n",
      "4  703305            24  2022-07-26 00:00:00   11225   \n",
      "\n",
      "                           bert_label_vec     gpt_label  \\\n",
      "0                               [0, 0, 0]  NO_STATEMENT   \n",
      "1  [0.020946145, 0.035621844, 0.94343203]  NO_STATEMENT   \n",
      "2                               [0, 0, 0]  NO_STATEMENT   \n",
      "3                               [0, 0, 0]  NO_STATEMENT   \n",
      "4                               [0, 0, 0]  NO_STATEMENT   \n",
      "\n",
      "           gpt_label_vec                                          text_feat  \\\n",
      "0  [0.98, 0.0, 0.0, 0.0]  [0.009678330272436142, 0.01754007115960121, -0...   \n",
      "1      [0.02, 0.08, 0.9]  [0.01969841495156288, 0.002665216103196144, 0....   \n",
      "2  [0.98, 0.0, 0.0, 0.0]  [-0.05713498219847679, -0.007283642888069153, ...   \n",
      "3  [0.98, 0.0, 0.0, 0.0]  [-0.0399835929274559, -0.01234126091003418, 0....   \n",
      "4  [0.98, 0.0, 0.0, 0.0]  [-0.01879699155688286, -0.00967487320303917, 0...   \n",
      "\n",
      "                         style_cluster_cats  \\\n",
      "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]   \n",
      "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]   \n",
      "2  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]   \n",
      "3  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]   \n",
      "4  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]   \n",
      "\n",
      "                                    topic_label_cats           sentiment_vec  \\\n",
      "0  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  [0.0, 0.85150373, 0.0]   \n",
      "1  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  [0.81784797, 0.0, 0.0]   \n",
      "2  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   [0.0, 0.0, 0.8913576]   \n",
      "3  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   [0.7862793, 0.0, 0.0]   \n",
      "4  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   [0.0, 0.5431884, 0.0]   \n",
      "\n",
      "                      activity_cluster_cats  \\\n",
      "0  [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0]   \n",
      "1  [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]   \n",
      "2  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]   \n",
      "3  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]   \n",
      "4  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]   \n",
      "\n",
      "               temporal_rhythm_cluster_cats  hate_vec  \\\n",
      "0  [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]  0.005426   \n",
      "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]  0.015105   \n",
      "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]  0.002214   \n",
      "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]  0.972849   \n",
      "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]  0.001269   \n",
      "\n",
      "               statement_vec  like_count_scaled  retruth_count_scaled  \\\n",
      "0  [0.0108067272230982, 0.0]          -0.739191             -0.493284   \n",
      "1   [0.0, 0.519844114780426]          -0.739191             -0.493284   \n",
      "2    [0.02990029938519, 0.0]          -0.399989             -0.493284   \n",
      "3   [0.393012136220932, 0.0]          -0.399989             -0.493284   \n",
      "4   [0.059361919760704, 0.0]          -0.739191             -0.493284   \n",
      "\n",
      "   reply_count_scaled                                        cluster_vec  \n",
      "0           -0.573825  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...  \n",
      "1           -0.573825  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...  \n",
      "2            0.552392  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...  \n",
      "3           -0.010717  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...  \n",
      "4           -0.573825  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...  \n"
     ]
    }
   ],
   "source": [
    "from io import text_encoding\n",
    "from regex import F\n",
    "\n",
    "\n",
    "print(feature_df.columns)\n",
    "print(feature_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "johan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
