{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1f268a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johan\\AppData\\Local\\Temp\\ipykernel_22132\\3374603742.py:6: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df1[\"label\"].replace(\"Mixed\", \"NO_STATEMENT\", inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NO_STATEMENT', 'FALSE', 'TRUE', 'MIXED']\n",
      "       id         label              label_vec\n",
      "0  703265  NO_STATEMENT  [0.98, 0.0, 0.0, 0.0]\n",
      "1  807621  NO_STATEMENT  [0.93, 0.0, 0.0, 0.0]\n",
      "2  703273  NO_STATEMENT  [0.98, 0.0, 0.0, 0.0]\n",
      "3  703274  NO_STATEMENT  [0.98, 0.0, 0.0, 0.0]\n",
      "4  703305  NO_STATEMENT  [0.98, 0.0, 0.0, 0.0]\n",
      "       id         label          label_vec\n",
      "0  807621  NO_STATEMENT  [0.02, 0.08, 0.9]\n",
      "1  703306  NO_STATEMENT  [0.05, 0.05, 0.9]\n",
      "2  703308         FALSE  [0.05, 0.6, 0.35]\n",
      "3  703394  NO_STATEMENT  [0.15, 0.15, 0.7]\n",
      "4  703282  NO_STATEMENT  [0.02, 0.08, 0.9]\n",
      "Index(['id', 'label', 'label_vec', 'gpt_label', 'gpt_label_vec'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from httpx import head\n",
    "import pandas as pd\n",
    "from regex import D\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "df1 = pd.read_csv('../Data/OriginalData/silver_labels_Timestamp5_7.csv')#chat gpt labels old\n",
    "df1[\"label\"].replace(\"Mixed\", \"NO_STATEMENT\", inplace=True)\n",
    "statement_cats = (df1[\"label\"].dropna().astype(str).unique().tolist())\n",
    "print(statement_cats)#what does mixed stand for?\n",
    "statement_enc = OneHotEncoder(categories=[statement_cats], sparse_output=False, handle_unknown=\"ignore\")\n",
    "statement_enc.fit(df1.loc[df1[\"label\"].notna(), [\"label\"]])\n",
    "onehot = statement_enc.transform(df1[[\"label\"]])\n",
    "conf = df1[\"confidence\"].fillna(0).to_numpy().reshape(-1, 1)\n",
    "df1[\"label_vec\"] = (onehot * conf).tolist()\n",
    "df1.drop(columns=[\"confidence\"], inplace=True)\n",
    "print(df1.head())\n",
    "df2 = pd.read_csv('../Data/OriginalData/truth_labels_prefilterd_gpt5.csv')#chat gpt labels after sentiment classification \n",
    "df2[\"label_vec\"] = df2[[\"TRUE\", \"FALSE\", \"NO_STATEMENT\"]].values.tolist()\n",
    "df2.rename(columns={\"truth_label\": \"label\"}, inplace=True)\n",
    "df2.drop(columns=[\"Unnamed: 0\", \"TRUE\", \"FALSE\", \"NO_STATEMENT\"], inplace=True)\n",
    "print(df2.head())\n",
    "\n",
    "df3 = pd.read_csv('../Data/OriginalData/truth_labels_fnn_gpt5.csv')\n",
    "df3.rename(columns={\"0\": \"id\",\"1\":\"label\"}, inplace=True)\n",
    "style_cluster_cats = (df3[\"label\"].dropna().astype(str).unique().tolist())\n",
    "style_enc = OneHotEncoder(categories=[style_cluster_cats], sparse_output=False, handle_unknown=\"ignore\")\n",
    "style_enc.fit(df3.loc[df3[\"label\"].notna(), [\"label\"]])\n",
    "onehot = style_enc.transform(df3[[\"label\"]])\n",
    "df3[\"label_vec\"] = onehot.tolist()\n",
    "\n",
    "\n",
    "df6 = df2.merge(df1, on=\"id\", how=\"outer\",)\n",
    "df6 = df6.merge(df3, on=\"id\", how=\"outer\")\n",
    "df6[\"gpt_label\"] = df6[\"label_x\"].combine_first(df6[\"label_y\"]).combine_first(df6[\"label\"]) \n",
    "df6[\"gpt_label_vec\"] = df6[\"label_vec_x\"].combine_first(df6[\"label_vec_y\"]).combine_first(df6[\"label_vec\"])\n",
    "df6 = df6.drop(columns=[\"label_x\", \"label_y\", \"label_vec_x\", \"label_vec_y\"])\n",
    "\n",
    "print(df6.columns)\n",
    "\n",
    "\n",
    "\n",
    "df4 = pd.read_csv('../Data/OriginalData/truths.tsv', sep=\"\\t\")\n",
    "df4 = df4[[\"id\", \"timestamp\"]]\n",
    "df5 = pd.read_csv('../Data/OriginalData/truthsocial_user_features.csv')\n",
    "df7 = pd.read_csv('../Data/OriginalData/truthsocial_features_new.csv')\n",
    "df5.set_index(\"id\")\n",
    "df7.set_index(\"id\")\n",
    "df5.update(df7[[\"topic_label\", \"style_cluster\", \"activity_cluster\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f0feb34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75131\n",
      "25020\n",
      "85300\n",
      "164733\n"
     ]
    }
   ],
   "source": [
    "print(len(df1))\n",
    "print(len(df2))\n",
    "print(len(df3))\n",
    "print(len(df6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "439ccdae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164733\n",
      "164737\n",
      "Index(['id', 'timestamp_id', 'author', 'like_count', 'retruth_count',\n",
      "       'reply_count', 'text', 'hate_pred', 'hate_prob', 'sentiment_id',\n",
      "       'sentiment_conf', 'sentiment', 'statement_flag',\n",
      "       'statement_probability', 'clusters', 'topic_label', 'style_cluster',\n",
      "       'activity_cluster', 'bert_label_vec', 'label', 'label_vec', 'gpt_label',\n",
      "       'gpt_label_vec', 'timestamp'],\n",
      "      dtype='object')\n",
      "Index(['id', 'label', 'label_vec', 'gpt_label', 'gpt_label_vec'], dtype='object')\n",
      "(2251,)\n"
     ]
    }
   ],
   "source": [
    "from httpx import head\n",
    "from polars import col\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "df5 = df5.rename(columns={\"timestamp\": \"timestamp_id\"})\n",
    "try:\n",
    "    df5[\"bert_label_vec\"] = df5[[\"TRUE\", \"FALSE\", \"NO_STMT\"]].values.tolist()\n",
    "    df5 = df5.drop(columns=[\"TRUTH_CLASS_x\",\"NO_STMT\",\"TRUE\" ,\"FALSE\"])#bert classifier output  \n",
    "except:\n",
    "    pass\n",
    "#try:\n",
    "    #df2 = df2.drop(columns=[\"Unnamed: 0\"])\n",
    "#except:\n",
    "#    pass\n",
    "df3 = df3.rename(columns={\"0\": \"id\",\"1\": \"truth_label_fnn\"})\n",
    "#df1 = df1.rename(columns=lambda c: c if c == \"id\" or c.startswith(\"silver_label_\") else f\"silver_label_{c}\")#these are chatgpt labels \n",
    "#try:\n",
    "#    df2[\"bert_prediction\"] = df2[[\"TRUE\", \"FALSE\", \"NO_STATEMENT\"]].values.tolist()#these are also chat gpt labels \n",
    "#    df2.drop(columns=[\"TRUE\", \"FALSE\", \"NO_STATEMENT\",\"truth_label\"], inplace=True)\n",
    "#except:\n",
    "#    pass\n",
    "print(len(df6))\n",
    "df = df5.merge(df6, on=\"id\", how=\"inner\")\n",
    "print(len(df))\n",
    "df[\"bert_label_vec\"] = df[\"bert_label_vec\"].apply(\n",
    "    lambda x: [0 if pd.isna(v) else v for v in x] if isinstance(x, list) else x\n",
    ")\n",
    "df=df.merge(df4, on=\"id\", how=\"inner\")\n",
    "#df = df.merge(df2, on=\"id\", how=\"inner\")\n",
    "#df = df.merge(df3, on=\"id\", how=\"inner\")\n",
    "print(df.columns)\n",
    "print(df6.columns)\n",
    "\n",
    "\n",
    "print(df[\"author\"].unique().shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f2d615a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    2.0\n",
      "1    2.0\n",
      "2    1.0\n",
      "3    1.0\n",
      "4    1.0\n",
      "5    1.0\n",
      "6    1.0\n",
      "7    1.0\n",
      "8    1.0\n",
      "9    1.0\n",
      "Name: activity_cluster, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df[\"activity_cluster\"][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b5a2613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Imports\n",
    "# =========================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "TEXT_DIM = 64          # node feature size for GNN\n",
    "MAX_LEN =2048\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# =========================\n",
    "# BERT + FFN Encoder\n",
    "# =========================\n",
    "class BertFFNEncoder(nn.Module):\n",
    "    def __init__(self, model_name, out_dim):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        \n",
    "        # freeze BERT\n",
    "        for p in self.bert.parameters():\n",
    "            p.requires_grad = False\n",
    "        \n",
    "        hidden_dim = self.bert.config.hidden_size\n",
    "        \n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        with torch.no_grad():\n",
    "            out = self.bert(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            cls_embedding = out.last_hidden_state[:, 0]  # [CLS]\n",
    "        \n",
    "        return self.ffn(cls_embedding)\n",
    "\n",
    "# =========================\n",
    "# Tokenizer & Model\n",
    "# =========================\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "encoder = BertFFNEncoder(MODEL_NAME, TEXT_DIM).to(DEVICE)\n",
    "encoder.eval()\n",
    "\n",
    "# =========================\n",
    "# Encode df[\"text\"]\n",
    "# =========================\n",
    "def encode_texts(texts, batch_size=32):\n",
    "    features = []\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "\n",
    "        enc = tokenizer(\n",
    "            batch,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=MAX_LEN,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        input_ids = enc[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = enc[\"attention_mask\"].to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            vecs = encoder(input_ids, attention_mask)\n",
    "\n",
    "        features.append(vecs.cpu().numpy())\n",
    "\n",
    "    return np.vstack(features)\n",
    "\n",
    "# =========================\n",
    "# RUN ENCODING\n",
    "# =========================\n",
    "text_features = encode_texts(df[\"text\"].tolist())\n",
    "\n",
    "# =========================\n",
    "# STORE AS NODE FEATURES\n",
    "# =========================\n",
    "#feature_df[\"text_feat\"] = text_features.tolist()\n",
    "\n",
    "# =========================\n",
    "# SANITY CHECKS\n",
    "# =========================\n",
    "#print(\"Node feature dim:\", len(feature_df[\"text_feat\"].iloc[0]))\n",
    "#print(\"Mean:\", np.mean(feature_df[\"text_feat\"].iloc[0]))\n",
    "#print(\"Std:\", np.std(feature_df[\"text_feat\"].iloc[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f37c2ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#print(feature_df[\"text_feat\"][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3cd6a6",
   "metadata": {},
   "source": [
    "known Columns: [id, timestamp, author, like_count, retruth_count, reply_count, text, hate_pred, hate_prob, sentiment_id, sentiment_conf, sentiment, style_cluster, activity_cluster, temporal_rhythm_cluster, topic_label, silver_label_label, silver_label_confidence, bert_prediction, truth_label_fnn]\n",
    "\n",
    "Unknown Columns: [ statement_flag, statement_probability, TRUTH_CLASS_x, NO_STMT, TRUE, FALSE, clusters,]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "853d0d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'timestamp_id', 'author', 'like_count', 'retruth_count',\n",
      "       'reply_count', 'text', 'hate_pred', 'hate_prob', 'sentiment_id',\n",
      "       'sentiment_conf', 'sentiment', 'statement_flag',\n",
      "       'statement_probability', 'clusters', 'topic_label', 'style_cluster',\n",
      "       'activity_cluster', 'bert_label_vec', 'label', 'label_vec', 'gpt_label',\n",
      "       'gpt_label_vec', 'timestamp'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3816e644",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'temporal_rhythm_cluster'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\johan\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'temporal_rhythm_cluster'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m feature_df= df[[\u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mtimestamp_id\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mtimestamp\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mauthor\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mbert_label_vec\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mgpt_label\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mgpt_label_vec\u001b[39m\u001b[33m\"\u001b[39m]].copy()\n\u001b[32m      7\u001b[39m feature_df[\u001b[33m\"\u001b[39m\u001b[33mtext_feat\u001b[39m\u001b[33m\"\u001b[39m] = text_features.tolist()\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m temporal_rhythm_cluster_cats = (\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemporal_rhythm_cluster\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m.dropna().astype(\u001b[38;5;28mstr\u001b[39m).unique().tolist())\n\u001b[32m     10\u001b[39m style_cluster_cats = (df[\u001b[33m\"\u001b[39m\u001b[33mstyle_cluster\u001b[39m\u001b[33m\"\u001b[39m].dropna().astype(\u001b[38;5;28mstr\u001b[39m).unique().tolist())\n\u001b[32m     11\u001b[39m activity_cluster_cats = (df[\u001b[33m\"\u001b[39m\u001b[33mactivity_cluster\u001b[39m\u001b[33m\"\u001b[39m].dropna().astype(\u001b[38;5;28mstr\u001b[39m).unique().tolist())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\johan\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4115\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\johan\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'temporal_rhythm_cluster'"
     ]
    }
   ],
   "source": [
    "from cv2 import norm\n",
    "import numpy as np\n",
    "from transformers import  AutoTokenizer\n",
    "from yaml import Token\n",
    "\n",
    "feature_df= df[[\"id\",\"timestamp_id\",\"timestamp\",\"author\", 'bert_label_vec',\"gpt_label\",\"gpt_label_vec\"]].copy()\n",
    "feature_df[\"text_feat\"] = text_features.tolist()\n",
    "\n",
    "temporal_rhythm_cluster_cats = (df[\"temporal_rhythm_cluster\"].dropna().astype(str).unique().tolist())\n",
    "style_cluster_cats = (df[\"style_cluster\"].dropna().astype(str).unique().tolist())\n",
    "activity_cluster_cats = (df[\"activity_cluster\"].dropna().astype(str).unique().tolist())\n",
    "topic_label_cats = (df[\"topic_label\"].dropna().astype(str).unique().tolist())\n",
    "sentiment_cats = (df[\"sentiment_id\"].dropna().astype(str).unique().tolist())\n",
    "sortcats = sorted(style_cluster_cats)\n",
    "print(sortcats)\n",
    "\n",
    "\n",
    "style_enc = OneHotEncoder(categories=[sortcats], sparse_output=False, handle_unknown=\"ignore\")\n",
    "style_enc.fit(df.loc[df[\"style_cluster\"].notna(), [\"style_cluster\"]])\n",
    "onehot = style_enc.transform(df[[\"style_cluster\"]])\n",
    "feature_df[\"style_cluster_cats\"] = onehot.tolist()\n",
    "\n",
    "topic_label_enc = OneHotEncoder(categories=[topic_label_cats], sparse_output=False, handle_unknown=\"ignore\")\n",
    "topic_label_enc.fit(df.loc[df[\"topic_label\"].notna(), [\"topic_label\"]])\n",
    "onehot = topic_label_enc.transform(df[[\"topic_label\"]])\n",
    "feature_df[\"topic_label_cats\"] = onehot.tolist()\n",
    "\n",
    "topic_label_enc = OneHotEncoder(categories=[topic_label_cats], sparse_output=False, handle_unknown=\"ignore\")\n",
    "topic_label_enc.fit(df.loc[df[\"topic_label\"].notna(), [\"topic_label\"]])\n",
    "onehot = topic_label_enc.transform(df[[\"topic_label\"]])\n",
    "feature_df[\"topic_label_cats\"] = onehot.tolist()\n",
    "\n",
    "\n",
    "sentiment_cats = sorted(sentiment_cats)\n",
    "setiment_enc = OneHotEncoder(categories=[sentiment_cats], sparse_output=False, handle_unknown=\"ignore\")\n",
    "setiment_enc.fit(df.loc[df[\"sentiment_id\"].notna(), [\"sentiment_id\"]])\n",
    "onehot = setiment_enc.transform(df[[\"sentiment_id\"]])\n",
    "conf = df[\"sentiment_conf\"].fillna(0).to_numpy().reshape(-1, 1)\n",
    "feature_df[\"sentiment_vec\"] = (onehot * conf).tolist()\n",
    "\n",
    "activity_cluster_cats = sorted(activity_cluster_cats)\n",
    "activity_cluster_enc = OneHotEncoder(categories=[activity_cluster_cats], sparse_output=False, handle_unknown=\"ignore\")\n",
    "activity_cluster_enc.fit(df.loc[df[\"activity_cluster\"].notna(), [\"activity_cluster\"]])         \n",
    "onehot = activity_cluster_enc.transform(df[[\"activity_cluster\"]])\n",
    "feature_df[\"activity_cluster_cats\"] = onehot.tolist()\n",
    "\n",
    "temporal_rhythm_cluster_cats = sorted(df[\"temporal_rhythm_cluster\"].dropna().astype(str).unique().tolist())\n",
    "temporal_rhythm_cluster_enc = OneHotEncoder(categories=[temporal_rhythm_cluster_cats], sparse_output=False, handle_unknown=\"ignore\")\n",
    "temporal_rhythm_cluster_enc.fit(df.loc[df[\"temporal_rhythm_cluster\"].notna(), [\"temporal_rhythm_cluster\"]])\n",
    "onehot = temporal_rhythm_cluster_enc.transform(df[[\"temporal_rhythm_cluster\"]])\n",
    "feature_df[\"temporal_rhythm_cluster_cats\"] = onehot.tolist()\n",
    "\n",
    "\n",
    "feature_df[\"hate_vec\"] = df[\"hate_prob\"]\n",
    "\n",
    "statement_cats = (df[\"statement_flag\"].dropna().astype(str).unique().tolist())\n",
    "statement_enc = OneHotEncoder(categories=[statement_cats], sparse_output=False, handle_unknown=\"ignore\")\n",
    "statement_enc.fit(df.loc[df[\"statement_flag\"].notna(), [\"statement_flag\"]])\n",
    "onehot = statement_enc.transform(df[[\"statement_flag\"]])\n",
    "conf = df[\"statement_probability\"].fillna(0).to_numpy().reshape(-1, 1)\n",
    "feature_df[\"statement_vec\"] = (onehot * conf).tolist()\n",
    "\n",
    "\n",
    "x = np.log1p(df[\"like_count\"])\n",
    "\n",
    "mean = x.mean()\n",
    "std = x.std()\n",
    "\n",
    "feature_df[\"like_count_scaled\"] = (x - mean) / std\n",
    "\n",
    "x = np.log1p(df[\"retruth_count\"])\n",
    "mean = x.mean()\n",
    "std = x.std()\n",
    "feature_df[\"retruth_count_scaled\"] = (x - mean) / std\n",
    "\n",
    "x = np.log1p(df[\"reply_count\"]) \n",
    "mean = x.mean()\n",
    "std = x.std()\n",
    "feature_df[\"reply_count_scaled\"] = (x - mean) / std\n",
    "\n",
    "#TODO: i need to deifine a dictionary \n",
    "#Tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "#feature_df[\"text_tokenized\"] = feature_df[\"id\"].apply(lambda x: Tokenizer.encode(df.loc[df[\"id\"] == x, \"text\"].values[0],))\n",
    "\n",
    "#feature_df[\"hate_prob\"] = df[\"hate_prob\"]\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# parse clusters\n",
    "parse = lambda x: [] if x is None or x != x else (\n",
    "    x if isinstance(x, list)\n",
    "    else [int(c) for c in str(x).strip(\"[]\").split(\",\") if c.strip()]\n",
    ")\n",
    "\n",
    "clusters = df[\"clusters\"].apply(parse)\n",
    "\n",
    "# vocab\n",
    "cats = sorted({c for row in clusters for c in row})\n",
    "idx = {c: i for i, c in enumerate(cats)}\n",
    "\n",
    "# encode + normalize\n",
    "feature_df[\"cluster_vec\"] = clusters.apply(\n",
    "    lambda row: (\n",
    "        lambda v: v / v.sum() if v.sum() > 0 else v\n",
    "    )(np.bincount([idx[c] for c in row], minlength=len(cats)).astype(float))\n",
    ").tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b304f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df.to_csv(\"../Data/ProcessedData/network_modeling_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae3abd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'timestamp_id', 'timestamp', 'author', 'bert_label_vec',\n",
      "       'gpt_label', 'gpt_label_vec', 'text_feat', 'style_cluster_cats',\n",
      "       'topic_label_cats', 'sentiment_vec', 'activity_cluster_cats',\n",
      "       'temporal_rhythm_cluster_cats', 'hate_vec', 'statement_vec',\n",
      "       'like_count_scaled', 'retruth_count_scaled', 'reply_count_scaled',\n",
      "       'cluster_vec'],\n",
      "      dtype='object')\n",
      "       id  timestamp_id            timestamp  author  \\\n",
      "0  703265            13  2022-05-14 00:00:00    2247   \n",
      "1  807621             6  2022-03-26 00:00:00   20054   \n",
      "2  703273            25  2022-08-01 00:00:00   11225   \n",
      "3  703274            25  2022-08-01 00:00:00   11225   \n",
      "4  703305            24  2022-07-26 00:00:00   11225   \n",
      "\n",
      "                           bert_label_vec     gpt_label  \\\n",
      "0                               [0, 0, 0]  NO_STATEMENT   \n",
      "1  [0.020946145, 0.035621844, 0.94343203]  NO_STATEMENT   \n",
      "2                               [0, 0, 0]  NO_STATEMENT   \n",
      "3                               [0, 0, 0]  NO_STATEMENT   \n",
      "4                               [0, 0, 0]  NO_STATEMENT   \n",
      "\n",
      "           gpt_label_vec                                          text_feat  \\\n",
      "0  [0.98, 0.0, 0.0, 0.0]  [-0.051763273775577545, 0.08814524114131927, -...   \n",
      "1      [0.02, 0.08, 0.9]  [0.05759405344724655, 0.07140514254570007, -0....   \n",
      "2  [0.98, 0.0, 0.0, 0.0]  [-0.02443370223045349, 0.029708588495850563, -...   \n",
      "3  [0.98, 0.0, 0.0, 0.0]  [-0.021199606359004974, 0.12760595977306366, -...   \n",
      "4  [0.98, 0.0, 0.0, 0.0]  [0.018650159239768982, 0.08332884311676025, -0...   \n",
      "\n",
      "                         style_cluster_cats  \\\n",
      "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]   \n",
      "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]   \n",
      "2  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]   \n",
      "3  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]   \n",
      "4  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]   \n",
      "\n",
      "                                    topic_label_cats           sentiment_vec  \\\n",
      "0  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  [0.0, 0.85150373, 0.0]   \n",
      "1  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  [0.81784797, 0.0, 0.0]   \n",
      "2  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   [0.0, 0.0, 0.8913576]   \n",
      "3  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   [0.7862793, 0.0, 0.0]   \n",
      "4  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   [0.0, 0.5431884, 0.0]   \n",
      "\n",
      "                      activity_cluster_cats  \\\n",
      "0  [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0]   \n",
      "1  [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]   \n",
      "2  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]   \n",
      "3  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]   \n",
      "4  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]   \n",
      "\n",
      "               temporal_rhythm_cluster_cats  hate_vec  \\\n",
      "0  [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]  0.005426   \n",
      "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]  0.015105   \n",
      "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]  0.002214   \n",
      "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]  0.972849   \n",
      "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]  0.001269   \n",
      "\n",
      "               statement_vec  like_count_scaled  retruth_count_scaled  \\\n",
      "0  [0.0108067272230982, 0.0]          -0.705218             -0.505684   \n",
      "1   [0.0, 0.519844114780426]          -0.705218             -0.505684   \n",
      "2    [0.02990029938519, 0.0]          -0.386270             -0.505684   \n",
      "3   [0.393012136220932, 0.0]          -0.386270             -0.505684   \n",
      "4   [0.059361919760704, 0.0]          -0.705218             -0.505684   \n",
      "\n",
      "   reply_count_scaled                                        cluster_vec  \n",
      "0           -0.548195  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...  \n",
      "1           -0.548195  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...  \n",
      "2            0.478021  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...  \n",
      "3           -0.035087  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...  \n",
      "4           -0.548195  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...  \n"
     ]
    }
   ],
   "source": [
    "from io import text_encoding\n",
    "from regex import F\n",
    "\n",
    "\n",
    "print(feature_df.columns)\n",
    "print(feature_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "johan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
