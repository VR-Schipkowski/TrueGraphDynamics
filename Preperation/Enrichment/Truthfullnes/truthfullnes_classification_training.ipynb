{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45d9ba7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\Users\\johan\\OneDrive\\Desktop\\TrueGraphDynamics\\Preperation\\Enrichment\\Truthfullnes\n",
      "(656365, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>author</th>\n",
       "      <th>like_count</th>\n",
       "      <th>retruth_count</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>text</th>\n",
       "      <th>hate_pred</th>\n",
       "      <th>hate_prob</th>\n",
       "      <th>sentiment_id</th>\n",
       "      <th>sentiment_conf</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>statement_flag</th>\n",
       "      <th>statement_probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>703265</td>\n",
       "      <td>13</td>\n",
       "      <td>2247</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>q+ be ready anons - public awakening coming - ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.005426</td>\n",
       "      <td>1</td>\n",
       "      <td>0.851504</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>0.010807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>807614</td>\n",
       "      <td>6</td>\n",
       "      <td>20054</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>enough is enough! retruth</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001936</td>\n",
       "      <td>1</td>\n",
       "      <td>0.489587</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>0.029001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>807619</td>\n",
       "      <td>7</td>\n",
       "      <td>20054</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@user</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001936</td>\n",
       "      <td>1</td>\n",
       "      <td>0.681377</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>0.037486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>807621</td>\n",
       "      <td>6</td>\n",
       "      <td>20054</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>joe biden wants this video removed from the in...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.015105</td>\n",
       "      <td>0</td>\n",
       "      <td>0.817848</td>\n",
       "      <td>negative</td>\n",
       "      <td>1</td>\n",
       "      <td>0.519844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>807622</td>\n",
       "      <td>6</td>\n",
       "      <td>20054</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;emoji: dart&gt;&lt;emoji: 100&gt;&lt;emoji: us&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003288</td>\n",
       "      <td>1</td>\n",
       "      <td>0.861522</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>0.011909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  timestamp  author  like_count  retruth_count  reply_count  \\\n",
       "0  703265         13    2247           0              0            0   \n",
       "1  807614          6   20054           0              0            0   \n",
       "2  807619          7   20054           0              0            0   \n",
       "3  807621          6   20054           0              0            0   \n",
       "4  807622          6   20054           0              0            0   \n",
       "\n",
       "                                                text  hate_pred  hate_prob  \\\n",
       "0  q+ be ready anons - public awakening coming - ...          0   0.005426   \n",
       "1                          enough is enough! retruth          0   0.001936   \n",
       "2                                              @user          0   0.001936   \n",
       "3  joe biden wants this video removed from the in...          0   0.015105   \n",
       "4               <emoji: dart><emoji: 100><emoji: us>          0   0.003288   \n",
       "\n",
       "   sentiment_id  sentiment_conf sentiment  statement_flag  \\\n",
       "0             1        0.851504   neutral               0   \n",
       "1             1        0.489587   neutral               0   \n",
       "2             1        0.681377   neutral               0   \n",
       "3             0        0.817848  negative               1   \n",
       "4             1        0.861522   neutral               0   \n",
       "\n",
       "   statement_probability  \n",
       "0               0.010807  \n",
       "1               0.029001  \n",
       "2               0.037486  \n",
       "3               0.519844  \n",
       "4               0.011909  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "print(os.getcwd())\n",
    "\n",
    "truth_df = pd.read_csv( r'..\\..\\..\\Data\\ProcessedData\\truth_cleand.csv')\n",
    "print(truth_df.shape)\n",
    "truth_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b49e49b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def clean_text(t):\n",
    "    if pd.isna(t):\n",
    "        return \"\"\n",
    "    t = str(t).lower()\n",
    "    t = re.sub(r\"http\\S+\", \"\", t)       # remove URLs\n",
    "    t = re.sub(r\"@\\w+\", \"@user\", t)     # normalize mentions\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip() # normalize whitespace\n",
    "    return t\n",
    "\n",
    "# clean text\n",
    "truth_df['text'] = truth_df['text'].apply(clean_text)\n",
    "\n",
    "# remove rows where text length < 20\n",
    "truth_df = truth_df[truth_df['text'].str.len() >= 20]\n",
    "\n",
    "# compute text lengths\n",
    "text_lengths = truth_df['text'].str.len()\n",
    "\n",
    "# histogram with bin width = 10\n",
    "bins = np.arange(0, text_lengths.max() + 10, 10)\n",
    "\n",
    "plt.hist(text_lengths, bins=bins)\n",
    "plt.xlabel('Text length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "265707b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           id  timestamp  author  like_count  retruth_count  reply_count  \\\n",
      "0      807621          6   20054           0              0            0   \n",
      "1      703306         24   11225          12              4            6   \n",
      "2      703308         24   11225          86             54           11   \n",
      "3      703394         23   11225          16              0            0   \n",
      "4      703282         24   11225           8              2            1   \n",
      "...       ...        ...     ...         ...            ...          ...   \n",
      "25015  934271          4   21566           0              0            0   \n",
      "25016  934269          4    1629         243            114            8   \n",
      "25017  934277          4    1629         161             72            8   \n",
      "25018  934275          4   21566           0              0            0   \n",
      "25019  934280          4    1629         299            149            7   \n",
      "\n",
      "                                                    text  hate_pred  \\\n",
      "0      joe biden wants this video removed from the in...          0   \n",
      "1      jesus act of mercy on him is not for me to arg...          0   \n",
      "2      our government lied..they never reported any s...          0   \n",
      "3      just like the windmills failed in texas snow s...          1   \n",
      "4      if god had not promised noah he would never de...          1   \n",
      "...                                                  ...        ...   \n",
      "25015  russian federation: we hear on the media of at...          0   \n",
      "25016  russian federation: we have found out a great ...          0   \n",
      "25017  represenative of brazil: any accusations of vi...          0   \n",
      "25018  represenative of brazil: any accusations of vi...          0   \n",
      "25019  russian federation: there was a un vehicle nea...          0   \n",
      "\n",
      "       hate_prob  sentiment_id  sentiment_conf sentiment  statement_flag  \\\n",
      "0       0.015105             0        0.817848  negative               1   \n",
      "1       0.001281             1        0.577395   neutral               1   \n",
      "2       0.024585             0        0.869792  negative               1   \n",
      "3       0.800656             0        0.942177  negative               1   \n",
      "4       0.825542             0        0.430576  negative               1   \n",
      "...          ...           ...             ...       ...             ...   \n",
      "25015   0.022433             0        0.774269  negative               1   \n",
      "25016   0.013165             0        0.696588  negative               1   \n",
      "25017   0.001482             1        0.833864   neutral               1   \n",
      "25018   0.001482             1        0.833864   neutral               1   \n",
      "25019   0.006454             0        0.671723  negative               1   \n",
      "\n",
      "       statement_probability  Unnamed: 0  TRUE  FALSE  NO_STATEMENT  \\\n",
      "0                   0.519844           0  0.02   0.08          0.90   \n",
      "1                   0.843496           1  0.05   0.05          0.90   \n",
      "2                   0.917135           2  0.05   0.60          0.35   \n",
      "3                   0.724487           3  0.15   0.15          0.70   \n",
      "4                   0.695649           4  0.02   0.08          0.90   \n",
      "...                      ...         ...   ...    ...           ...   \n",
      "25015               0.742752       25015  0.05   0.65          0.30   \n",
      "25016               0.845719       25016  0.10   0.60          0.30   \n",
      "25017               0.756471       25017  0.60   0.05          0.35   \n",
      "25018               0.756471       25018  0.60   0.05          0.35   \n",
      "25019               0.935813       25019  0.10   0.20          0.70   \n",
      "\n",
      "        truth_label  \n",
      "0      NO_STATEMENT  \n",
      "1      NO_STATEMENT  \n",
      "2             FALSE  \n",
      "3      NO_STATEMENT  \n",
      "4      NO_STATEMENT  \n",
      "...             ...  \n",
      "25015         FALSE  \n",
      "25016         FALSE  \n",
      "25017          TRUE  \n",
      "25018          TRUE  \n",
      "25019  NO_STATEMENT  \n",
      "\n",
      "[25020 rows x 19 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>author</th>\n",
       "      <th>like_count</th>\n",
       "      <th>retruth_count</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>text</th>\n",
       "      <th>hate_pred</th>\n",
       "      <th>hate_prob</th>\n",
       "      <th>sentiment_id</th>\n",
       "      <th>sentiment_conf</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>statement_flag</th>\n",
       "      <th>statement_probability</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>TRUE</th>\n",
       "      <th>FALSE</th>\n",
       "      <th>NO_STATEMENT</th>\n",
       "      <th>truth_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>807621</td>\n",
       "      <td>6</td>\n",
       "      <td>20054</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>joe biden wants this video removed from the in...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.015105</td>\n",
       "      <td>0</td>\n",
       "      <td>0.817848</td>\n",
       "      <td>negative</td>\n",
       "      <td>1</td>\n",
       "      <td>0.519844</td>\n",
       "      <td>0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.90</td>\n",
       "      <td>NO_STATEMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>703306</td>\n",
       "      <td>24</td>\n",
       "      <td>11225</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>jesus act of mercy on him is not for me to arg...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001281</td>\n",
       "      <td>1</td>\n",
       "      <td>0.577395</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>0.843496</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.90</td>\n",
       "      <td>NO_STATEMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>703308</td>\n",
       "      <td>24</td>\n",
       "      <td>11225</td>\n",
       "      <td>86</td>\n",
       "      <td>54</td>\n",
       "      <td>11</td>\n",
       "      <td>our government lied..they never reported any s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.024585</td>\n",
       "      <td>0</td>\n",
       "      <td>0.869792</td>\n",
       "      <td>negative</td>\n",
       "      <td>1</td>\n",
       "      <td>0.917135</td>\n",
       "      <td>2</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.35</td>\n",
       "      <td>FALSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>703394</td>\n",
       "      <td>23</td>\n",
       "      <td>11225</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>just like the windmills failed in texas snow s...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.800656</td>\n",
       "      <td>0</td>\n",
       "      <td>0.942177</td>\n",
       "      <td>negative</td>\n",
       "      <td>1</td>\n",
       "      <td>0.724487</td>\n",
       "      <td>3</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.70</td>\n",
       "      <td>NO_STATEMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>703282</td>\n",
       "      <td>24</td>\n",
       "      <td>11225</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>if god had not promised noah he would never de...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.825542</td>\n",
       "      <td>0</td>\n",
       "      <td>0.430576</td>\n",
       "      <td>negative</td>\n",
       "      <td>1</td>\n",
       "      <td>0.695649</td>\n",
       "      <td>4</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.90</td>\n",
       "      <td>NO_STATEMENT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  timestamp  author  like_count  retruth_count  reply_count  \\\n",
       "0  807621          6   20054           0              0            0   \n",
       "1  703306         24   11225          12              4            6   \n",
       "2  703308         24   11225          86             54           11   \n",
       "3  703394         23   11225          16              0            0   \n",
       "4  703282         24   11225           8              2            1   \n",
       "\n",
       "                                                text  hate_pred  hate_prob  \\\n",
       "0  joe biden wants this video removed from the in...          0   0.015105   \n",
       "1  jesus act of mercy on him is not for me to arg...          0   0.001281   \n",
       "2  our government lied..they never reported any s...          0   0.024585   \n",
       "3  just like the windmills failed in texas snow s...          1   0.800656   \n",
       "4  if god had not promised noah he would never de...          1   0.825542   \n",
       "\n",
       "   sentiment_id  sentiment_conf sentiment  statement_flag  \\\n",
       "0             0        0.817848  negative               1   \n",
       "1             1        0.577395   neutral               1   \n",
       "2             0        0.869792  negative               1   \n",
       "3             0        0.942177  negative               1   \n",
       "4             0        0.430576  negative               1   \n",
       "\n",
       "   statement_probability  Unnamed: 0  TRUE  FALSE  NO_STATEMENT   truth_label  \n",
       "0               0.519844           0  0.02   0.08          0.90  NO_STATEMENT  \n",
       "1               0.843496           1  0.05   0.05          0.90  NO_STATEMENT  \n",
       "2               0.917135           2  0.05   0.60          0.35         FALSE  \n",
       "3               0.724487           3  0.15   0.15          0.70  NO_STATEMENT  \n",
       "4               0.695649           4  0.02   0.08          0.90  NO_STATEMENT  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weak_labels = pd.read_csv(r'..\\..\\..\\Data\\ProcessedData\\truth_labels_prefilterd_gpt5.csv')\n",
    "df_joined = pd.merge(truth_df, weak_labels, on='id', how='inner')\n",
    "print(df_joined)\n",
    "df_joined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23f0e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\johan\\.venv\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e813b9f9e5454ac895f8a6d50348ceb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25020 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='6256' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   4/6256 01:25 < 74:18:16, 0.02 it/s, Epoch 0.00/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 164\u001b[39m\n\u001b[32m    155\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m    156\u001b[39m \u001b[38;5;66;03m# Trainer\u001b[39;00m\n\u001b[32m    157\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m    158\u001b[39m trainer = Trainer(\n\u001b[32m    159\u001b[39m     model=model,\n\u001b[32m    160\u001b[39m     args=training_args,\n\u001b[32m    161\u001b[39m     train_dataset=dataset\n\u001b[32m    162\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m    167\u001b[39m \u001b[38;5;66;03m# Save\u001b[39;00m\n\u001b[32m    168\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m    169\u001b[39m model.save_pretrained(OUTPUT_DIR)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\johan\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\johan\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2674\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2667\u001b[39m context = (\n\u001b[32m   2668\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2669\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2670\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2671\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2672\u001b[39m )\n\u001b[32m   2673\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2674\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2676\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2677\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2678\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2679\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2680\u001b[39m ):\n\u001b[32m   2681\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2682\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\johan\\.venv\\Lib\\site-packages\\transformers\\trainer.py:4020\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n\u001b[32m   4017\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m   4019\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m4020\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4022\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[32m   4023\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   4024\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4025\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps == \u001b[32m0\u001b[39m\n\u001b[32m   4026\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\johan\\.venv\\Lib\\site-packages\\transformers\\trainer.py:4110\u001b[39m, in \u001b[36mTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m   4108\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_items_in_batch\u001b[39m\u001b[33m\"\u001b[39m] = num_items_in_batch\n\u001b[32m   4109\u001b[39m     inputs = {**inputs, **kwargs}\n\u001b[32m-> \u001b[39m\u001b[32m4110\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4111\u001b[39m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[32m   4112\u001b[39m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[32m   4113\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.past_index >= \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\johan\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\johan\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\johan\\.venv\\Lib\\site-packages\\accelerate\\utils\\operations.py:819\u001b[39m, in \u001b[36mconvert_outputs_to_fp32.<locals>.forward\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(*args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m819\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\johan\\.venv\\Lib\\site-packages\\accelerate\\utils\\operations.py:807\u001b[39m, in \u001b[36mConvertOutputsToFp32.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    806\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m807\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\johan\\.venv\\Lib\\site-packages\\torch\\amp\\autocast_mode.py:44\u001b[39m, in \u001b[36mautocast_decorator.<locals>.decorate_autocast\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_autocast\u001b[39m(*args, **kwargs):\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 107\u001b[39m, in \u001b[36mDualHeadModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, is_statement, is_true)\u001b[39m\n\u001b[32m    105\u001b[39m mask = is_statement == \u001b[32m1\u001b[39m\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mask.any():\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m     truth_targets = \u001b[43mis_true\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m     truth_logits_masked = truth_logits[mask]\n\u001b[32m    110\u001b[39m     weights = torch.where(\n\u001b[32m    111\u001b[39m         truth_targets == \u001b[32m0\u001b[39m,\n\u001b[32m    112\u001b[39m         torch.tensor(FALSE_WEIGHT, device=truth_logits.device),\n\u001b[32m    113\u001b[39m         torch.tensor(\u001b[32m1.0\u001b[39m, device=truth_logits.device)\n\u001b[32m    114\u001b[39m     )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Config\n",
    "# -----------------------------\n",
    "MODEL_NAME = \"microsoft/deberta-v3-large\"\n",
    "MAX_LEN = 512\n",
    "OUTPUT_DIR = \"./truthfulness_dual_head_deberta\"\n",
    "\n",
    "FALSE_WEIGHT = 3.0\n",
    "\n",
    "# -----------------------------\n",
    "# Load DataFrame\n",
    "# -----------------------------\n",
    "# REQUIRED COLUMNS: [\"text\", \"truth_label\"]\n",
    "\n",
    "label_map = {\n",
    "    \"TRUE\": (1, 1),\n",
    "    \"FALSE\": (1, 0),\n",
    "    \"NO_STATEMENT\": (0, -1)\n",
    "}\n",
    "\n",
    "df_joined[[\"is_statement\", \"is_true\"]] = df_joined[\"truth_label\"].apply(\n",
    "    lambda x: pd.Series(label_map[x])\n",
    ")\n",
    "\n",
    "dataset = Dataset.from_pandas(\n",
    "    df_joined[[\"text\", \"is_statement\", \"is_true\"]]\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Tokenizer\n",
    "# -----------------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize(example):\n",
    "    return tokenizer(\n",
    "        example[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "dataset = dataset.map(tokenize, batched=True)\n",
    "dataset.set_format(\n",
    "    type=\"torch\",\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"is_statement\", \"is_true\"]\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Base model (fp16, GPU)\n",
    "# -----------------------------\n",
    "base_model = AutoModel.from_pretrained(\n",
    "    MODEL_NAME,\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Dual-head model\n",
    "# -----------------------------\n",
    "class DualHeadModel(nn.Module):\n",
    "    def __init__(self, base_model, hidden_size):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.statement_head = nn.Linear(hidden_size, 1)\n",
    "        self.truth_head = nn.Linear(hidden_size, 1)\n",
    "\n",
    "        self.stmt_loss_fn = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "        self.truth_loss_fn = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids,\n",
    "        attention_mask,\n",
    "        is_statement=None,\n",
    "        is_true=None\n",
    "    ):\n",
    "        outputs = self.base_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "\n",
    "        # DeBERTa uses CLS token at position 0\n",
    "        cls = outputs.last_hidden_state[:, 0]\n",
    "\n",
    "        stmt_logits = self.statement_head(cls).squeeze(-1)\n",
    "        truth_logits = self.truth_head(cls).squeeze(-1)\n",
    "\n",
    "        loss = None\n",
    "        if is_statement is not None:\n",
    "            is_statement = is_statement.float()\n",
    "\n",
    "            stmt_loss = self.stmt_loss_fn(\n",
    "                stmt_logits, is_statement\n",
    "            ).mean()\n",
    "\n",
    "            mask = is_statement == 1\n",
    "            if mask.any():\n",
    "                truth_targets = is_true[mask].float()\n",
    "                truth_logits_masked = truth_logits[mask]\n",
    "\n",
    "                weights = torch.where(\n",
    "                    truth_targets == 0,\n",
    "                    torch.tensor(FALSE_WEIGHT, device=truth_logits.device),\n",
    "                    torch.tensor(1.0, device=truth_logits.device)\n",
    "                )\n",
    "\n",
    "                truth_loss_raw = self.truth_loss_fn(\n",
    "                    truth_logits_masked, truth_targets\n",
    "                )\n",
    "\n",
    "                truth_loss = (truth_loss_raw * weights).mean()\n",
    "            else:\n",
    "                truth_loss = torch.tensor(\n",
    "                    0.0, device=stmt_logits.device\n",
    "                )\n",
    "\n",
    "            loss = stmt_loss + truth_loss\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"p_statement\": torch.sigmoid(stmt_logits),\n",
    "            \"p_true\": torch.sigmoid(truth_logits)\n",
    "        }\n",
    "\n",
    "model = DualHeadModel(\n",
    "    base_model,\n",
    "    base_model.config.hidden_size\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Training arguments\n",
    "# -----------------------------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=4,   # you can try 32\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=4,\n",
    "    learning_rate=2e-5,\n",
    "    fp16=True,\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"epoch\",\n",
    "    optim=\"adamw_torch\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Trainer\n",
    "# -----------------------------\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# -----------------------------\n",
    "# Save\n",
    "# -----------------------------\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "johan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
